# ğŸ”— Web Link Scraper

Un outil puissant et tres rapide pour extraire et classifier tous les liens d'un site web avec une analyse rÃ©cursive en profondeur.

## ğŸ“‹ Table des matiÃ¨res

- [FonctionnalitÃ©s](#-fonctionnalitÃ©s)
- [Installation](#-installation)
- [Utilisation](#-utilisation)
- [Classification des liens](#-classification-des-liens)
- [Structure des rÃ©sultats](#-structure-des-rÃ©sultats)
- [Exemples](#-exemples-de-sortie-console)
- [Configuration avancÃ©e](#-configuration-avancÃ©e)
- [DÃ©pannage](#-dÃ©pannage)
- [Contribution](#-contribution)
- [Licence](#-licence)

## âœ¨ FonctionnalitÃ©s

- ğŸš€ **Scraping rÃ©cursif** : Exploration en profondeur des sites web
- ğŸ“‚ **Classification automatique** : Organisation des liens par type (HTML, documents, images, etc.)
- ğŸ” **DÃ©tection intelligente** : DiffÃ©renciation entre liens internes et externes
- ğŸ“Š **Statistiques dÃ©taillÃ©es** : Rapport complet sur les liens trouvÃ©s
- ğŸ’¾ **Export JSON** : Sauvegarde structurÃ©e des rÃ©sultats
- ğŸ›¡ï¸ **Gestion SSL** : Support des sites HTTPS avec certificats invalides
- âš¡ **Performance optimisÃ©e** : Headers rÃ©alistes pour Ã©viter les blocages
- ğŸ¯ **Filtrage intelligent** : Exclusion automatique des liens non pertinents

## ğŸ“¦ Installation

### PrÃ©requis

- Go 1.16 ou supÃ©rieur
- Git

### Ã‰tapes d'installation

1. **Cloner le repository**
```bash
git clone https://github.com/alaminedione/web-link-scraper.git
cd web-link-scraper
```


2. **Compiler le projet**
```bash
go mod tidy
go build -o link-scraper main.go
```

## ğŸš€ Utilisation

### Syntaxe de base

```bash
./link-scraper <URL> [max_depth] [output_folder]
```

### ParamÃ¨tres

| ParamÃ¨tre | Description | Valeur par dÃ©faut |
|-----------|-------------|-------------------|
| `URL` | L'URL du site web Ã  analyser | *Obligatoire* |
| `max_depth` | Profondeur maximale de rÃ©cursion | `1` |
| `output_folder` | Dossier de sauvegarde des rÃ©sultats | `./scraping_results` |

### Exemples d'utilisation

**Scraping simple (profondeur 1)**
```bash
./link-scraper https://example.com
```

**Scraping en profondeur**
```bash
./link-scraper https://example.com 3
```

**Scraping avec dossier de sortie personnalisÃ©**
```bash
./link-scraper https://example.com 2 ./mes-resultats
```

## ğŸ“Š Classification des liens

Le scraper classe automatiquement les liens trouvÃ©s dans les catÃ©gories suivantes :

### ğŸ“„ Pages HTML
- Extensions : `.html`, `.htm`, `.php`, `.asp`, `.aspx`, `.jsp`
- Pages web classiques et dynamiques

### ğŸ“‘ Documents
- Extensions : `.pdf`, `.doc`, `.docx`, `.xls`, `.xlsx`, `.ppt`, `.pptx`
- Fichiers bureautiques et documents

### ğŸ–¼ï¸ Images
- Extensions : `.jpg`, `.jpeg`, `.png`, `.gif`, `.svg`, `.webp`
- Tous types d'images web

### âš™ï¸ Scripts
- Extensions : `.js`, `.mjs`, `.ts`
- Fichiers JavaScript et TypeScript

### ğŸ¨ Feuilles de style
- Extensions : `.css`, `.scss`, `.sass`, `.less`
- Fichiers de style

### ğŸ¬ MultimÃ©dia
- Extensions : `.mp4`, `.mp3`, `.avi`, `.mov`, `.wav`
- Fichiers audio et vidÃ©o

### ğŸ“¦ Archives
- Extensions : `.zip`, `.rar`, `.7z`, `.tar`, `.gz`
- Fichiers compressÃ©s

### â“ Autres
- Tous les autres types de fichiers

## ğŸ“ Structure des rÃ©sultats

Les rÃ©sultats sont sauvegardÃ©s dans un dossier horodatÃ© :

```
scraping_results/
â””â”€â”€ example_com_20240127_143022/
    â”œâ”€â”€ summary.json          # RÃ©sumÃ© complet
    â”œâ”€â”€ html_pages.json       # Liste des pages HTML
    â”œâ”€â”€ documents.json        # Liste des documents
    â”œâ”€â”€ images.json          # Liste des images
    â”œâ”€â”€ scripts.json         # Liste des scripts
    â”œâ”€â”€ stylesheets.json     # Liste des CSS
    â”œâ”€â”€ multimedia.json      # Liste des mÃ©dias
    â””â”€â”€ archives.json        # Liste des archives
```

### Format du fichier summary.json

```json
{
  "base_url": "https://example.com",
  "total_links": 150,
  "internal_links": ["..."],
  "external_links": ["..."],
  "classified_links": {
    "html_pages": [...],
    "documents": [...],
    "images": [...]
  },
  "category_summary": {
    "html_pages": 45,
    "documents": 12,
    "images": 78
  },
  "statistics": {
    "pages_visited": 25,
    "execution_time": "1m23s",
    "max_depth_reached": 3
  },
  "timestamp": "2024-01-27 14:30:22"
}
```

## ğŸ–¥ï¸ Exemples de sortie console

```
ğŸš€ Starting ultra-fast scraping of: https://example.com
ğŸ“Š Maximum depth: 2
ğŸ’¾ Output directory: ./scraping_results
--------------------------------------------------
ğŸ”— Testing connection to https://example.com...
âœ… Connection successful (Status: 200)
ğŸ” [Depth 0] Scraping: https://example.com
âœ… Page loaded successfully: https://example.com
ğŸ“Š Total of 45 links found on this page
ğŸ” [Depth 1] Scraping: https://example.com/about
âœ… Page loaded successfully: https://example.com/about
ğŸ“Š Total of 23 links found on this page

==================================================
ğŸ“Š DETAILED STATISTICS
==================================================
ğŸŒ Website: https://example.com
â±ï¸  Execution Time: 15.2s
ğŸ“„ Pages Visited: 15
ğŸ”— Total Links: 234
ğŸ  Internal Links: 189
ğŸŒ External Links: 45
ğŸ“Š Max Depth Reached: 2

ğŸ“‚ LINKS BY CATEGORY:
   ğŸ“„ Html_pages: 67
   ğŸ“‘ Documents: 23
   ğŸ–¼ï¸ Images: 89
   âš™ï¸ Scripts: 12
   ğŸ¨ Stylesheets: 8
   ğŸ¬ Multimedia: 5

ğŸ’¾ Results saved to: ./scraping_results/example_com_20240127_143022
âœ… Scraping completed successfully!
```

## âš™ï¸ Configuration avancÃ©e

### Modification des catÃ©gories

Pour ajouter ou modifier les catÃ©gories de fichiers, Ã©ditez la variable `fileExtensions` dans le code :

```go
var fileExtensions = map[LinkCategory][]string{
    CategoryHTML:       {".html", ".htm", ".php"},
    CategoryDocument:   {".pdf", ".doc", ".docx"},
    // Ajoutez vos extensions ici
}
```

### ParamÃ¨tres de timeout

Pour modifier le timeout des requÃªtes HTTP :

```go
client := &http.Client{
    Transport: tr,
    Timeout:   30 * time.Second, // Modifier ici
}
```

## ğŸ”§ DÃ©pannage

**Erreur SSL/TLS**
- Le scraper ignore automatiquement les erreurs de certificat SSL
- Pour dÃ©sactiver cette fonctionnalitÃ©, modifiez `InsecureSkipVerify: false`

**Timeout sur sites lents**
- Augmentez la valeur du timeout dans la configuration du client HTTP

**Blocage par le serveur**
- Le scraper utilise des headers rÃ©alistes pour Ã©viter la dÃ©tection
- Vous pouvez ajouter un dÃ©lai entre les requÃªtes si nÃ©cessaire


## ğŸ¤ Contribution

Les contributions sont les bienvenues ! Pour contribuer :

1. Fork le projet
2. CrÃ©ez votre branche (`git checkout -b feature/AmazingFeature`)
3. Committez vos changements (`git commit -m 'Add some AmazingFeature'`)
4. Push vers la branche (`git push origin feature/AmazingFeature`)
5. Ouvrez une Pull Request

## ğŸ“ Licence

Ce projet est sous licence MIT. Voir le fichier [LICENSE](LICENSE) pour plus de dÃ©tails.


---

DÃ©veloppÃ© avec â¤ï¸ en Go
